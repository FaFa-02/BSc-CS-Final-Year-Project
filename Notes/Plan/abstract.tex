\section{Abstract}
For most people buying a house will be one of their most important and expensive economic decisions that they will take in their lives.\cite{Lida_R_Weinstock} Because of this it would be logical to say that being able to accurately predict the prices of said houses would be of extreme value to people. One possible way to make these predictions would be to create a machine learning model that, given a certain amount of features from each house, would be able to create an accurate prediction of their price. There are a wide range of different machine learning algorithms that could be used to solve this problem. However, this project will focus on two: Ridge Regression and Decision Trees.

Within machine learning there are two main methodologies utilised when trying to solve a problem, supervised and unsupervised learning. Unsupervised learning is used when the data being analysed does not need to be labelled, but instead needs to be sorted into groups by their features. On the other hand, supervised learning is concerned with providing labels to unlabelled data in an dataset, such as a list of houses without a price attached to them. Within supervised learning there is once again two different types of problems that occur: classification and regression problems. Classification problems occur when the list of possible classifications is finite such as identifying a handwritten digit as its correct number. However, in the Boston housing problem the list of possible labels is infinite as they could be any price, i.e., any real number.\cite{Volodymyr_Vovk_ch2} This is called a regression problem and will be the focus of the research paper.

Regression itself can be used to determine a causal relationship between independent and dependent variables. This is then expanded to create regression models which focus on using the independent variable '\(x\)' to predict the dependent variable '\(y\)'.\cite{Maulud_Abdulazeez_2020} With the Boston housing problem the independent variable \(x\) are the features of each house such as, per capita crime rates per town, while the dependent variable \(y\) are the prices of the houses.\cite{Shashank_Gupta} 

There are several different regression algorithms that can be used to solve the Boston housing problem one of them being the Ridge Regression algorithm which will be the focus of the report. However, the performance of another algorithm, Decision Trees, will also be analysed and compared to see which one is more effective at solving the Boston Housing problem. 

Ridge Regression a technique that builds off of the simpler linear regression algorithm that uses the equation \(y=X\beta+\epsilon\) to model the predictions. Here \(y\) is vector of size \(N\times1\), N being the number of unlabelled houses, i.e., our dependent variables. \(X\) is a matrix of size \(N\times K\), K being the number of independent variables. \(beta\) is a vector of size \(K\times1\) which contains the regression coefficients for the model. \(\epsilon\) is a vector of size \(N\times1\) which represent the error terms.\cite{Adam_Hayes, Marco_Taboga} In Linear Regression the least-squares method is used to find the value of the regression coefficients. However, data can suffer from multicollinearity which is when the independent variables in the model are correlated. This is a problem as the idea behind the regression coefficient is to represent a change in the dependant variable for each change in the independent variable when all other independent variables stay the same. If they are correlated then it becomes harder to change one without changing the other, this causes there to be big differences between the coefficients depending on what independent variables are included in the model making them very sensitive to small changes and causing overfitting problems.\cite{Jim_Frost} 

Overfitting occurs when a model is has a high level of accuracy on the training dataset, but performs badly when it comes to the testing set, i.e., high variance.\cite{Jaiganesh_Nagidi} Ridge Regression counteracts this by applying regularisation to the model, specifically L2 Regularisation. This is done by adding a small amount of bias into the model which in return decreases the amount of variance which caused the poor performance. This means that the model will lose some of its accuracy but will provide better performance in the long term by being more reliable.\cite{Cory_Maklin} L2 regularisation achieves this by enhancing the Sum of Squares Error equation which is used in linear regression to judge the performance of the model, the smaller the result the more optimised the model is \[ SSE=\sum_{i=1}^{n} (y_i-\hat{y_i})^{2} \], where \(y_i\) are the values in the dataset and \(\hat{y_i}\) are the predicted values\cite{Ted_Hessing, Saed_Sayad_RR}. The way Ridge regression enhances this is by adding a penalty term at the end of the equation which is the summation of squared weights of each feature, then multiplied by lambda to define how harsh the penalty is\cite{Kerem_Kargin} \[ SSE_{L2}=\sum_{i=1}^{n} (y_i-\hat{y_i})^{2} +\lambda\sum_{j=1}^P \beta_j^2.\] After this the regression coefficients are optimised by finding the values which will generate the smallest \(SSE_{L2}\) possible, finalising the model.

The Decision Trees algorithm can be used to solve both classification and regression problems. It works by breaking the dataset down into smaller and smaller subsets whilst at the same time developing a decision tree, which is composed of decision nodes and leaf nodes. Decision nodes always have two or more branches which are possible values for the attribute being tested. Leaf nodes contain a numerical value which corresponds to the dependent variable.\cite{Saed_Sayad_DT}

In this project I plan on implementing both of these algorithms and comparing their performance on one another to see which one is more effective at resolving the Boston housing problem. I will also be using 2 different datasets, one with a high number of features and another with a low number. Then I will run each algorithm on both datasets to see how this affects their results as well. 





